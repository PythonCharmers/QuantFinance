{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Quantitative Finance\n",
    "\n",
    "Copyright (c) 2019 Python Charmers Pty Ltd, Australia, <https://pythoncharmers.com>. All rights reserved.\n",
    "\n",
    "<img src=\"img/python_charmers_logo.png\" width=\"300\" alt=\"Python Charmers Logo\">\n",
    "\n",
    "Published under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. See `LICENSE.md` for details.\n",
    "\n",
    "Sponsored by Tibra Global Services, <https://tibra.com>\n",
    "\n",
    "<img src=\"img/tibra_logo.png\" width=\"300\" alt=\"Tibra Logo\">\n",
    "\n",
    "\n",
    "## Module 2.2: Modelling Techniques\n",
    "\n",
    "### 2.2.1 Data for Linear Regression Models\n",
    "\n",
    "In this module we will go through the Linear Regression models again in more detail, before reviewing ARMA and then enhancing it with ARIMA and GARCH. We will revisit the previous considered concepts from a more theoretical view over these two modules, and afterwards we will go back to the normal mixture of code and theory when we investigate ARIMA.\n",
    "\n",
    "\n",
    "### Representing Data\n",
    "\n",
    "There are four major types of data and measurements, which can become a variable in a model. However, some need to be handled differently than simply used as-is. The four major types are:\n",
    "\n",
    "* Ratio, which allows us to multiply values in the range\n",
    "* Interval, which doesn't allow multiplying, but allows averaging\n",
    "* Ordinal, which doesn't allow averaging but allows order\n",
    "* Nominal, also called Categorical, which do not have order\n",
    "\n",
    "Note that some authors use different names, conventions or types, but the general pattern is very consistent across multiple sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio Data\n",
    "\n",
    "The first type we will look at is **Ratio** data, which are standard numbers. They are measurements with a range, and those measurements have a clear distance between them.\n",
    "\n",
    "An example of an interval is a stock's price. It can be measured, and the differences between prices can be computed. For instance:\n",
    "\n",
    "* Company A's stock is valued at \\$100\n",
    "* Company B's stock is valued at \\$105\n",
    "\n",
    "We can therefore say it costs $5 more to buy a stock in Company B than it does in Company A. Further, we can say that Company B's stock is 5\\% more than Company A's stock. (If this is all seeming a bit obvious, counter examples of data that you can't do this with are coming up).\n",
    "\n",
    "A Ratio doesn't need to encapsulate all information about the samples. For instance, in the above, we don't take into consideration things such as the number of stocks in a company, the ownership proportion, or the movement in the stock price. That's fine - we are just talking about a variable, and not a full model.\n",
    "\n",
    "The next important aspect a Ratio variable has is a \"clear zero\", which makes comparisons possible of the form \"X is twice Y\". For instance, if Company C's stock was valued at $200, we can say it costs \"twice as much\" as Company A to buy. Note that ratios can be negative in some cases (not in stock prices though, although stock valuations could be negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval Data\n",
    "\n",
    "**Interval** data works a lot like Ratio data. You can compute the mean and median of both, the data is obviously ordered, and you can say \"X is more than Y\".\n",
    "\n",
    "However, Interval data doesn't have a clear zero reference, known as a true zero, which makes comparisons like \"X is twice Y\" meaningless. For instance, Temperature is interval data.\n",
    "\n",
    "If Melbourne has a temperature of 15째C, and Sydney has a temperature of 30째C, we can say that Sydney has a greater temperature than Melbourne. We **cannot** say that Sydney is twice as hot at Melbourne. \n",
    "\n",
    "While this would make intuitive sense in lay speak, it actually doesn't mean anything. The reason? The units matter. If we measure in Fahrenheit, we get Melbourne's temperature as 59째F and Sydney's as 86째F, which is no longer twice, despite the underlying facts not changing.\n",
    "\n",
    "Interval data is still very useful, and you can normally just deal with it as a ratio value. This is especially true with practical data where the data sits in a normal range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Answer the following questions intuitively first, and then either research or compute the actual answers to confirm.\n",
    "\n",
    "1. Does the ratio of price of two stocks (X is 105% of Y) change if you convert the prices from USD to AUD before doing the comparison?\n",
    "2. Is temperature in Kelvin a Ratio or an Interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs 1\n",
    "# - as long as u are using the same currency for both stocks, the ratio does not change\n",
    "# - the absolute values though would change\n",
    "\n",
    "# qs 2\n",
    "# - kelvin is a ratio, as u cannot have negative kelvin, hence all reference to it is to 0 K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/ratio_interval.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Data\n",
    "\n",
    "As Interval data is like Ratio data with restrictions, **Ordinal** data is a \"restricted\" Interval. The data is ordered, but not really numerical in nature. \n",
    "\n",
    "An example is the Wong-Baker FACES Pain Rating Scale, used for helping children describe how much pain they are in to a doctor or nurse.\n",
    "\n",
    "<img src=\"img/Wong_Baker_Scale.sr.JPG\">\n",
    "<small>Source: https://en.wikipedia.org/wiki/File:Wong_Baker_Scale.sr.JPG</source>\n",
    "\n",
    "Note that even though you may not be able to read the text, you could still describe the level of pain you are in.\n",
    "\n",
    "The data is clearly ordered - a person is in more pain on level 6 of the scale than level 2. However, you wouldn't say they are in \"three times more pain\". Further, you could compute the average pain someone is in, but that value is ultimately meaningless - the different between a 4 and a 6 is not really the same as the difference between a 6 and an 8. Instead, it would make more sense to compute the median or mode, and say \"the patient normally answers around 6\", rather than say \"the average pain rating is 6.3\".\n",
    "\n",
    "Another example is the standard feedback \"How satisfied are you with our service?\", with values ranging from \"Very Unsatisfied\" to \"Very Satisfied\". There is a clear order - one would rather customers be Satisfied than Unsatisfied, and \"Very Satisfied\" is even better! However, there are not really numerical values you can assign clearly to this data. \n",
    "\n",
    "Note that some people *do* assign numerical numbers, and *do* compute the mean value - this does have some use, but is ultimately meaningless, for instance, in comparing two survey results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal data (Categorical)\n",
    "\n",
    "**Nominal** data, also called Categorical or Labels, are unordered categories of data. For instance, if I asked if your pet was a Dog or a Cat, there isn't a \"greater\" of these values (one could argue with this point, but not mathematically).\n",
    "\n",
    "A common type of data here is Gender. Options could include \"Female\", \"Male\" and \"Other\". They could be presented in an order, and could even get numbered labels for storing, but you can't compute the average. You *can* compute the ratio of the total (e.g. 53% Female, 46% Male, 1% Other), but it would make no sense to say the average customer is 53% Female. If anything, that sends a very different message!\n",
    "\n",
    "A two-option nominal value is also called a dichotomous variable. This could be True/False, Correct/Incorrect, Married/Not Married, and other data of the form X / not X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "What type of variable is the following?\n",
    "\n",
    "1. the age of a person\n",
    "2. the time taken for a program to run\n",
    "3. Level of education obtained, out of \"University degree\", \"No Education\", \"High school completed\", \"Some schooling\".\n",
    "4. A variable noting which doctor performed the surgery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs 1\n",
    "# ratio as u cannot have negative age, so all reference is to 0\n",
    "\n",
    "# qs 2\n",
    "# ratio as u cannot have negative time, so all reference is to 0\n",
    "\n",
    "# qs 3\n",
    "# ordinal, as it can be ordered from worse to best, kind of like a ordered factor in R\n",
    "\n",
    "# qs 4\n",
    "# nominal, categorical data which is exactly like a factor in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/ordinal_nominal.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with nominal and ordinal data types\n",
    "\n",
    "In statistics and programming, the programs we write often assume data is at least Interval, and perhaps Ratio. We freely compute the mean of data (even if that doesn't make sense) and report the result. One of the key problems here is that we often use numbers to encode variables. For instance, in our gender question, we might assign the following numbers:\n",
    "\n",
    "* Male: 0\n",
    "* Female: 1\n",
    "* Other: 2\n",
    "\n",
    "Therefore, our data looks like this: `gender = [0, 0, 1, 1, 1, 0, 2]`, indicating Male, Male, Female, Female, and so on. This means that technically we can compute `np.mean(gender)`, and that operation will work. Further, we could fit a OLS model on this data, and that will probably also show something.\n",
    "\n",
    "To better encode the gender variable, a one-hot encoding is normally recommended. This expands the data into multiple variables of the form \"Is Male?\", \"Is Female?\" and \"Is Other?\". Only, and exactly, one of these three will be 1, and the others will be always 0. This turns this into a dichotomous nominal variable, which can actually be used as a ratio!\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Gender is <i>nearly</i> dichotomous for the general population - most people identify as either male or female. You can do things with dichotomous variables like compute the ratio from the mean, but if you do, you will run the risk of your results being meaningless in some samples.\n",
    "</div>\n",
    "\n",
    "\n",
    "You can encode ordinal data the same way. You do lose some data doing this though, as it turns your ordinal data into nominal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Review the documentation for Scikit-learn's OrdinalEncoder and OneHotEncoder transformers. What do they do?\n",
    "\n",
    "\n",
    "#### Extended Exercise\n",
    "\n",
    "1. What is the problem with the name of Scikit-Learn's Ordinal Encoder?\n",
    "1. Load some data that contains ordinal and nominal variables (e.g. Boston house prices). Convert the ordinal and nominal variables to encoded forms using the above scikit-learn classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Under $100_000  $100_000 - $199_999  $200_000 - $299_999  \\\n",
      "Date                                                                   \n",
      "1990-08-31            60.0                 23.0                 10.0   \n",
      "1993-11-30            54.0                 27.0                  7.0   \n",
      "1993-12-31            53.0                 28.0                  9.0   \n",
      "1994-01-31            57.0                 21.0                  9.0   \n",
      "1994-02-28            55.0                 29.0                  7.0   \n",
      "\n",
      "            $300_000 - $399_999  $400_000 - $499_999  $500_000 +  \n",
      "Date                                                              \n",
      "1990-08-31                  1.0                  0.0         1.0  \n",
      "1993-11-30                  3.0                  1.0         3.0  \n",
      "1993-12-31                  2.0                  1.0         1.0  \n",
      "1994-01-31                  4.0                  0.0         1.0  \n",
      "1994-02-28                  2.0                  2.0         2.0  \n",
      "[[40. 12.  4.  0.  0.  0.]\n",
      " [37. 16.  1.  2.  1.  2.]\n",
      " [36. 17.  3.  1.  1.  0.]\n",
      " ...\n",
      " [ 4.  2. 10. 12. 14. 28.]\n",
      " [ 2.  3. 11. 12. 14. 28.]\n",
      " [ 0.  0. 13. 16. 13. 30.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# qs 1\n",
    "# sklearn.preprocessing.OrdinalEncoder\n",
    "# - The input to this transformer should be an array-like of integers or strings, denoting the values taken on by \n",
    "#       categorical (discrete) features. The features are converted to ordinal integers. \n",
    "\n",
    "# sklearn.preprocessing.OneHotEncoder\n",
    "# - Encode categorical features as a one-hot numeric array.\n",
    "# - this is used in deep learning\n",
    "\n",
    "# extended exercise\n",
    "# qs 1\n",
    "# it convertes it into a ordered categorical variable, but the order may not be in the sequence u want\n",
    "\n",
    "# qs 2\n",
    "# copied from solutions\n",
    "import quandl\n",
    "import my_secrets\n",
    "quandl.ApiConfig.api_key = my_secrets.QUANDL_API_KEY\n",
    "\n",
    "# \n",
    "housing_prices = quandl.get(\"UMICH/SOC22-University-of-Michigan-Consumer-Survey-Current-Market-Value-of-Primary-Residence\")\n",
    "housing_prices = housing_prices.iloc[:,0:6] #Trim data down to price categories only\n",
    "print(housing_prices.head())\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(housing_prices)\n",
    "ord_prices = ord_enc.transform(housing_prices)\n",
    "print(ord_prices)\n",
    "\n",
    "hot_enc = OneHotEncoder(categories='auto')\n",
    "hot_enc.fit(housing_prices)\n",
    "hot_enc = hot_enc.transform(housing_prices).toarray()\n",
    "print(hot_enc)\n",
    "\n",
    "# ordinal encoder tells u which category each data point belongs to\n",
    "# 1-hot encoder arranges it in a sparse matrix such that the value is 1 at the index of the category (where the categories are arranged by index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions see `solutions/ordinal_encoding.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats for data\n",
    "\n",
    "Our data is usually called $X$, specifically a 2 dimensional matrix, where the rows correspond to each sample in our dataset (like a stock data for a day, a company, or a person), and columns correspond to features, or variables (such as the closing price, the number of employees or the person's height).\n",
    "\n",
    "Therefore, value $X_{i, j}$ is the value of the $j$th variable for the $i$th sample.\n",
    "\n",
    "Normally, we use the term $n$ to describe how many samples we have, and $k$ to describe how many variables. If no other information is given, assume these values. That said, lots of other terms are used as well (for instance, $m$ is usually used if a second set of data is included for the number of samples in that second set).\n",
    "\n",
    "Therefore, our matrix $X$ has shape $n \\times k$.\n",
    "\n",
    "Next, we have our coefficients, which are $\\beta$, and this is a value for each variable, so it has shape $k$. Often, it is much more useful for this to be a 2D array of size $k \\times 1$. This allows us to compute the values $X\\beta$, which gives a $n \\times 1$ value, which is $Y$.\n",
    "\n",
    "\n",
    "#### Question\n",
    "\n",
    "If $u$ is the error term associated with each sample, what would its shape be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs\n",
    "# - for each sample there is an error term \n",
    "# - so if Y is n x 1, then y is also n x 1\n",
    "# - or it could just be n if 1 column. but if there is more than 1 column, we need to use n x n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/error_term_u.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples versus Population, and Notation\n",
    "\n",
    "To date, these modules have been a little slack with terminology relating to samples versus populations. We will address that here before we continue further.\n",
    "\n",
    "If we remember our equation for our Linear Regression model, it has been presented so far like this:\n",
    "\n",
    "$Y =  X \\beta + u$\n",
    "\n",
    "In this case, we have $X$ as our independent variables in the format above. $\\beta$ is the set of *actual parameters for mapping the linear relationship*. In other words, if there was a perfect computer simulation of the world, the relationship between $X$ and $y$ would be perfectly modelled by this equation. Note that we may not know these true values - we may never know these actual values. (They may not actually exist either, but the assumptions behind a linear model assume they do.)\n",
    "\n",
    "Finally, $u$ is the error of the model, which really is just what is left over from taking the values of $\\beta$, and then computing $u = Y - \\beta X$. These are the errors, but they are the *actual population errors*, again that are quite theoretical and you may never know what these are. Further, the values of $u$ are not learned, they are the remaining error. It may seem odd that we need $u$ if $\\beta$ is perfectly defined, but note that sometimes the same values for $x$ may result in a different $y$ value. If the $\\beta$ values are fixed, then the only way we can distinguish is through $u$.\n",
    "\n",
    "The above equations deal with ground truth population data, however almost always you are working with sample data. This means that we don't have all $X$ values. This means that, at best, we can estimate the true values for $\\beta$, so we use the notation $\\hat{\\beta}$ to denote we have an estimation of $\\beta$, not the true values.\n",
    "\n",
    "As we do not know the true values for $\\beta$, the errors we get in the model are just estimated too, so we use the notation $\\hat{u}$. Then, finally, we come to our $Y$ values being estimated, so we use the notation $\\hat{Y}$ to describe them, or $\\hat{y}$ to describe the predicted value for a given $y$ value.\n",
    "\n",
    "So while OLS is aiming to get as close to the true values as possible, what we are really learning is:\n",
    "\n",
    "$\\hat{Y} = X \\hat{\\beta} + \\hat{u}$\n",
    "\n",
    "Other notation may use $\\textbf{b}$ as the learned and estimated values for the true values of $\\beta$.\n",
    "\n",
    "This confuses the notation to keep doing this, and have separate symbols for effectively the same thing. For these notebooks, we won't be too particular about the specific symbols and \"hats\" for them. Know however, that if you ever publish a journal paper in an academic journal, you'll need to ensure you have all the symbols correctly identified.\n",
    "\n",
    "At the end of the day, notation is just notation - simply describe what your variables are upfront if you do not know the normal terminology to use.\n",
    "\n",
    "#### Question\n",
    "\n",
    "Why is $\\beta$ presented after $X$ in the previous equations, and not on the left? Hint: they are matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X dimensions is n x k, B dimensions is k x 1, hence needs to be that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs\n",
    "# - order is important in matrix operations\n",
    "# - X - n x k\n",
    "# - B is k x 1\n",
    "# - so the cols of x need to match the rows of B for matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/dot.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Prediction\n",
    "\n",
    "We will finish this module with a mid-point revision of some of the basics covered so far, but specifically with a view to prediction in the Linear Regression space.\n",
    "\n",
    "The most basic, reasonable, prediction algorithm is simply to predict the mean of your sample of data. This is the best estimator if you have just one variable. i.e. if we have *just* the heights for lots of dogs, and want to do the best guess of the next dog's height, we just use the mean. This is mathematically proven as the best estimate you can get - known as the *expected value*.\n",
    "\n",
    "Our model is therefore of the form:\n",
    "\n",
    "$\\hat{y} = \\bar{x} + u$\n",
    "\n",
    "Where $\\bar{x}$ is the mean of $X$ and $u$ is the errors, the residuals. These residuals are the difference between the prediction and the actual value. You'll often hear this referred to as a measure of the \"Goodness of fit\".\n",
    "\n",
    "The sum of squared error (SSE) is a commonly used metric in comparing two models to determine which is better. It is the sum of the squared of these residuals, therefore:\n",
    "\n",
    "$SSE = \\sum{u^2}$\n",
    "\n",
    "For a single set of values, the mean is the value that minimises the SSE in a single row of data. This is why, according to this model, the mean is the best predictor if we have no other data.\n",
    "\n",
    "The goal of linear regression is to minimise the SSE when we use multiple independent variables to predict a dependent variable. This is only one evaluation metric - there are dozens of commonly used ones, and for financial data we might be more concerned with \"absolute profit\" or some other metric more closely tied to business outcomes. Use the evaluation metric that works for your application. SSE has some nice algorithmic properties, for instance, you can compute the gradient at all points, allowing for solving the OLS algorithm quickly to minimise SSE. Not all metrics are as nice.\n",
    "\n",
    "When you fit any model to data, and want to evaluate how well it does in practice, you must fit your model on one set of data, and evaluate it on another set of data. This is known as a train/test split. You fit your data on the training data, and evaluate on the testing data. A common split is simply to split randomly 1/3 of the data for testing, and the remaining 2/3 for training. The exact numbers usually don't matter too much. If you are using a time series dataset, you'll need to split by time rather than randomly. Think always about how your model will be used in practice - for price prediction, you want to be predicting tomorrow's price, not some random day in the past. Therefore, your model needs to be evaluated when it's trying to predict data in the future.\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "An issue with train/test splits, is that you must, by definition, lose the learning power of 1/3 of your dataset (or whatever you put into the test split). To address this, use cross-validation.\n",
    "\n",
    "Review the documentation for scikit-learn's cross-validation functions at https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "What does cross-validation do, and why is it better than a simple train/test split?\n",
    "\n",
    "Why is there an *additional* test split in the diagram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple train test splits, can use smaller test sets depending on sample size, and average the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs 1\n",
    "# - cross validation trains and tests your model on all combinations of train-test splits\n",
    "# - using 1 split would cause some potential biase as you might have picked good or bad subsets\n",
    "# - running X-validation would ensure your model is trained and tested on the entire dataset\n",
    "# - for 10 fold X-validation you would test ur model on all combinations of 9:1 splits (train-test)\n",
    "# - this way you iterate ur model through all data\n",
    "\n",
    "# qs 2\n",
    "# - we want to develop a robust model first using cross-validation for training-validation\n",
    "# - testing should be done on an independent dataset to further evaluate our model\n",
    "# - this is why data is usually split into train-val-test sets, where validation set is used for model tuning\n",
    "# - you never tunr ur model on the test set directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/cross_validation_one.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4dc6b0224d27235dec988ead42f740f3c8067d08028a6940d609f8d318d6bb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
