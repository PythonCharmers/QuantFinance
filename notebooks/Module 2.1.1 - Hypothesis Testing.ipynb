{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Quantitative Finance\n",
    "\n",
    "Copyright (c) 2019 Python Charmers Pty Ltd, Australia, <https://pythoncharmers.com>. All rights reserved.\n",
    "\n",
    "<img src=\"img/python_charmers_logo.png\" width=\"300\" alt=\"Python Charmers Logo\">\n",
    "\n",
    "Published under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. See `LICENSE.md` for details.\n",
    "\n",
    "Sponsored by Tibra Global Services, <https://tibra.com>\n",
    "\n",
    "<img src=\"img/tibra_logo.png\" width=\"300\" alt=\"Tibra Logo\">\n",
    "\n",
    "\n",
    "## Module 2.1: Hypothesis Testing\n",
    "\n",
    "### 2.1.1 Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is a formal method of testing your assumptions with data and statistics. With hypothesis testing, we create two (or more) competing hypothesis and decide which is more likely. In a variety of circumstances, we consider something \"unlikely\" if it has less than 5% chance of happening, but note this would mean that this happens in 1 in 20 experiments *anyway, just due to chance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/significant.png\" title=\"'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'\" alt=\"Significant\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Above comic from https://xkcd.com/882/ Hint: count how many \"per-colour\" experiments were performed.)\n",
    "\n",
    "A common hypothesis pair is the following:\n",
    "\n",
    "* $H_0$, the Null hypothesis, is that any difference between the data in our sample, and the general population, is caused strictly by chance.\n",
    "* $H_A$, the Alternative hypothesis, is that there is a significant difference.\n",
    "\n",
    "The hypotheses must be mutually exclusive, as in it cannot be that both are true. They do not have to be exhaustive, i.e. account for all possible scenarios, but it is often the case. It is usually easier to compute the probability for $H_0$, i.e. $P(H_0)$ and then just compute $P(H_A) = 1 - P(H_0)$. This only applies if the pair is exhaustive.\n",
    "\n",
    "For example, we might have a sample, say trading firms using statistical analysis for decision making. Our Null hypothesis is that these firms are no different to the *population*, which may be either \"all trading firms\" or \"random people picking stocks\". We set up an experiment (more below) and find that we have a 4% chance that our Null hypothesis would generate results this extreme.\n",
    "\n",
    "We might then say that, because the chance is so low, we reject the Null hypothesis that any different is strictly by chance. \n",
    "\n",
    "Note that in this example, there is still a one in 25 chance of obtaining the result purely through \"luck\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, this does *not* mean that we accept that using statistical analysis causing trading firms to profit more than normal. There may be some other factors involved that are causing the difference. While this might sound like [weasel words](https://en.wikipedia.org/wiki/Weasel_word), this type of inference happens all the time. For instance, firms using statistical analysis might get their advantage from simply a more careful and thorough analysis, rather than the specific statistical analysis they apply. They might be run by larger firms (who can afford the extra staff to do the statistical analysis and have access to more data), or many other factors.\n",
    "\n",
    "It could also be simply by chance, as the above comic demonstrates.\n",
    "\n",
    "This is a common logical fallacy called *Denying the antecedent*:\n",
    "    \n",
    "    If P, then Q.\n",
    "    Therefore, if not P, then not Q.\n",
    "\n",
    "Note that the conclusion above is **invalid** based solely on the condition. Our statistical tests generally tell us \"not P\", and we are left with a bit more evidence of \"not Q\", but never proof.\n",
    "\n",
    "Be wary of this fallacy. Statistics is often pessimistic in this regard - it rarely tells you that you are correct, it generally just tells you if there is a strong chance you are wrong.\n",
    "\n",
    "\n",
    "Another common pitfall, demonstrated by the above comic, is that when we run multiple tests at the same time, our notion of a probability threshold must change, otherwise it becomes **likely** that we observe a purely-by-chance outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Suppose we had $n$ independent tests. In each test, there is a 5% likelihood that the effect we are measuring is \"high\", for however that is defined. If we run all $n$ tests, what is the likelihood that *at least one test* measures a positive, high, outcome, for given $n$ values:\n",
    "\n",
    "- Two experiments\n",
    "- Ten experiments\n",
    "- Twenty experiments\n",
    "\n",
    "The above results can be computed with a calculator (or by hand if you are fine with leaving a fraction).\n",
    "\n",
    "#### Extended exercise\n",
    "\n",
    "Write a program with a function that does the following:\n",
    "\n",
    "- Compute 1000 random numbers from a normal distribution $N(0, 1)$\n",
    "- Compute the mean of those values\n",
    "\n",
    "Note the expected mean is 0.\n",
    "\n",
    "Plot a histogram of the means from running this function 10,000 times. How many of the results have a mean of more than 0.166?\n",
    "\n",
    "From your results here, note that even with *purely random data*, we can get very high differences between a same (running our function once) and the general population ($N(0, 1)$), just by chance in our we got our sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/hypothesis_one.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of hypotheses\n",
    "\n",
    "Any test of a hypothesis starts with a formal declaration of the hypothesis and its assumptions. Some general assumptions are things like \"tests are independent of each other\", \"sample drawn at random from the population\" and other factors that might reduce any latent causes that aren't being tested.\n",
    "\n",
    "The Null hypothesis is a \"business as normal\" hypothesis. The new medicine doesn't work. The new strategy didn't make a difference to sales. The sample doesn't differ from the population.\n",
    "\n",
    "The Alternative hypothesis is that our intervention caused some change. For instance, the new medicine reduces illness. Sales increased significantly from the new strategy. The sample is different from the population.\n",
    "\n",
    "Normally we are interested in computing some statistic and then identifying what the likelihood is of that statistic having occurred by chance, based on our assumptions.\n",
    "\n",
    "A commonly used method here is a p-test, where we are testing if a mean is different (>, <, $\\neq$) to the population mean, when we assume the mean from a sample is otherwise drawn from a normal distribution. For instance, if we roll 100 dice, we get an expected value of 350, and a normal distribution of results centred around this value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "dice_rolls = np.random.randint(1, 7, size=100)\n",
    "print(dice_rolls.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.array([np.random.randint(1, 7, size=100).sum() for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sums, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we suspected a dice to be \"loaded\", that is, it is more likely to come up with a given number, we might run an experiment. We might suspect this dice to roll higher numbers more frequently than lower numbers. Our hypothesis is:\n",
    "\n",
    "$H_0$: The dice has a true expected value of 3.5\n",
    "\n",
    "$H_A$: The dice has an expected value of significantly more than 3.5\n",
    "\n",
    "\n",
    "We roll this suspect dice 100 times, and compute the mean. We get a total value of 410. We then do this experiment 3 more times, getting values of 420, 400, 405. This is unlikely, as it is an expected value of around 4.1. This would fit with our hypothesis, but how unlikely is it? We will look into this further in a later module, but for now, we can look up this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t stat: 13.7602, p value: 0.000831\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = stats.ttest_1samp([410, 420, 400, 405], 350)\n",
    "print('t stat: {:.4f}, p value: {:4f}'.format(t_stat, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-stat, while important, isn't really of value here. It is an intermediate statistic we then use to compute the p-value, which is what is normally needed here. The p-value is simply a probability, which you can get in any valid method you can think of. In this case, it is a statistical test \"what is the probability of getting a t stat of this size by chance?\".\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. If we assume a confidence level of 0.05 qualifies as \"significant\", what can we say about our hypothesis?\n",
    "2. As a percentage, what is the likelihood of the given samples being obtained by chance?\n",
    "3. Can we suggest that the dice rolls values of 6 more frequently than other values?\n",
    "4. Review the documentation for all of the `scipy.stats.ttest_???` functions and identify when each would be needed.\n",
    "\n",
    "\n",
    "Note there are also T tests in the `statsmodels` package, which can be called in a similar way.\n",
    "\n",
    "\n",
    "#### Extended Exercise\n",
    "\n",
    "Create an alternative hypothesis and experiment to address question 3 above. How can we test if a dice rolls 6s more frequently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/inferring_statistics.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common problems and issues\n",
    "\n",
    "### Large P values\n",
    "\n",
    "If you do not get a lower p-value, you do not have a strong result. We cannot accept the null hypothesis, we just \"fail to reject\" it.\n",
    "\n",
    "If we were to run our above statistic and get a p value of 0.2 (assuming a threshold of significance of 0.05), we do not \"accept the null hypothesis\", we simply fail to reject it at our significance level.\n",
    "\n",
    "\n",
    "### Thresholds\n",
    "\n",
    "It is important also to ensure the p value threshold is set *before* the experiment, and not after you get your results. If you do not, it is tempting to just \"change\" the threshold after you get your results. This removes any independence you had in your result, and the experimenter is effectively arbitrarily changing the result - why bother doing the test in this case?\n",
    "\n",
    "A common value for the threshold is 0.05. There is no basis in this value, it is just \"what is generally used\". If your hypothesis is a matter of life or death, this value is probably too high. If it is of no great consequence, it might be too low. Reason about the value before blindly accepting on.\n",
    "\n",
    "Also note that people are generally terrible dealing with low probabilities. The difference between a false positive of 0.02 and one of 0.01 is \"one in fifty\" compared to \"one in one hundred\". That's twice as likely for the first. Typically, it can be better to estimate with an intuitive \"one in ...\" amount, then convert that to a percentage.\n",
    "\n",
    "\n",
    "### Multiple Simultaneous experiments\n",
    "\n",
    "With modern computers able to run simulations continuously, and at scale, a common issue arising is that the p-value thresholds most commonly used (i.e. 0.05) are only valid for individual tests.\n",
    "\n",
    "For example, suppose we had the following hypothesis:\n",
    "\n",
    "$H_0$: Stock price changes are random for IBM on Mondays\n",
    "\n",
    "$H_A$: Stock prices are more likely to drop on Mondays.\n",
    "\n",
    "\n",
    "We might test this hypothesis and get a p value of 0.2, indicating there is insufficient evidence to reject the null hypothesis, and therefore do not automatically buy on Tuesdays after the drop.\n",
    "\n",
    "We might then consider \"does this pattern hold on any other day?\". So we consider the same hypothesis, but for Tuesday, Wednesday, Thursday and Friday. We find the following significance levels:\n",
    "\n",
    "* Monday: 0.2\n",
    "* Tuesday: 0.1\n",
    "* Wednesday: 0.8\n",
    "* Thursday: 0.04\n",
    "* Friday: 0.5\n",
    "\n",
    "Aha! Thursdays have a *highly significant* effect. We fail to reject all other null hypothesis, accept the Thursday one, and start trading. \n",
    "\n",
    "What went wrong?\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. Research the \"Multiple comparisons problem\" and identify a way to fix our hypothesis. We still want to check if any day has a significant value for our hypothesis, but we want to do it in a rigorous way.\n",
    "2. Does our finding hold after adjusting? The solution uses one specific method of fixing the thresholds - if you choose another, then you may get another answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/multiple_comparisons.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "A topic we will get into in more detail later, but a useful one to touch on here, is the use of simulations for computing p values. When testing a hypothesis, you can use a simulation of your null hypothesis, and then with that simulation, estimate the likelihood of findings like your sample. For instance:\n",
    "\n",
    "$H_0$: The AUD/USD change is a random walk (that is, there is no pattern)\n",
    "\n",
    "$H_A$: The AUD/USD change tends to follow the previous change with lag 1 (that is, if the previous change was up, it is more likely this change will be up too)\n",
    "\n",
    "\n",
    "A little more formally, if we ignore \"no change\", we might say that $p$, the proportion of changes that are consistent with the previous change, is 50%, when we have:\n",
    "\n",
    "$H_0: p = 0.5$\n",
    "\n",
    "$H_A: p > 0.5$\n",
    "\n",
    "\n",
    "To do this, we might analyse some data and find that we get a proportion with a value of 0.6, that is, the proportion of times that a change follows the previous change is 0.6 (60%). Is this \"significant\"?\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">That value above is artificial - we compute the real data in the exercise later</div>\n",
    "\n",
    "To test this, we can create a simulation. In this simulation, we are focused on our null hypothesis - that there is no relationship between a change and the previous one. We might run our experiment for one year's worth of data (i.e. 365 changes), with each change randomly and uniformly chosen from \"up\" or \"down\". We then measure the proportion values to get our result from the simulation. Repeat many times, and you can then use this to estimate the p-value, or the probability that the null hypothesis is true.\n",
    "\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "1. Download the USD/AUD prices from Quandl\n",
    "2. Identify whether each change is \"up\" or \"down\" and compute the sample proportion value (it was 0.6 in the artificial data above)\n",
    "\n",
    "#### Extended Exercise\n",
    "\n",
    "1. Create and run the simulation mentioned above, where we simulate a random walk scenario and compute the proportion of times a change corresponds with the previous change.\n",
    "2. Run the simulation many times\n",
    "3. Compute the p value and determine whether to accept or reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/hypothesis_two.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for specific attributes\n",
    "\n",
    "Often it is of use to know that data has a certain property, for instance it is normally distributed, correlated, or whether two samples are basically the same or not.\n",
    "\n",
    "\n",
    "### Correlation between variables\n",
    "\n",
    "These tests are designed to test whether two samples of data are independent of each other, or if a dependency exists:\n",
    "\n",
    "$H_0$ the two samples tested are independent from each other\n",
    "\n",
    "$H_A$ the two samples tested have a dependency between them\n",
    "\n",
    "* Spearman's Rank Correlation, implemented as `scipy.stats.spearmanr`\n",
    "* Pearson's correlation coefficient, implemented as `scipy.stats.pearsonr`\n",
    "* Chi-Squared test, implemented as `scipy.stats.chi2_contingency` and `statsmodels.stats.proportion.proportions_chisquare`\n",
    "\n",
    "\n",
    "### Gaussian Distribution Tests\n",
    "\n",
    "There are a few tests designed to test that a distribution is Gaussian (normal). They include:\n",
    "\n",
    "* The Shapiro-Wilk test, implemented as `scipy.stats.shapiro`\n",
    "* D’Agostino’s $K^2$, implemented as `scipy.stats.normaltest`\n",
    "* Kolmogorov-Smirnov, implemented as `scipy.stats.kstest` and `statsmodels.stats.diagnostic.kstest_normal`\n",
    "* Anderson-Darling, implemented as `scipy.stats.anderson`\n",
    "\n",
    "Each of the above tests against the hypothesis:\n",
    "\n",
    "$H_0$ the data has a Gaussian distribution\n",
    "\n",
    "$H_A$ the data does not have a Gaussian distribution\n",
    "\n",
    "\n",
    "### Are two samples equal?\n",
    "\n",
    "These tests assert that, given two samples, they are effectively equal (i.e. they came from the same distribution):\n",
    "\n",
    "* Student's t-test, as identified earlier, implemented in quite a few methods in both scipy and statsmodels\n",
    "* Analysis of Variance Test (ANOVA), `scipy.stats.f_oneway` and `statsmodels.api.stats.anova_lm` (among a few other ways to call it).\n",
    "* Mann-Whitney U Test, implemented as `scipy.stats.manwhitneyu`\n",
    "* Wilcoxon Signed Rank Test, implemented as `scipy.stats.wilcoxon`\n",
    "\n",
    "\n",
    "Note that while the tests in these categories have the same purpose, they are not the same in terms of quality, speed, and even coding signatures! Always check the documentation for the function you are using first, before using it in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extended Exercise\n",
    "\n",
    "Using a simulation, create your own function that can compute the t-test and p values for a Student's t-test.\n",
    "\n",
    "For a comparison of two independent samples (i.e. \"here are two samples, do they come from the same distribution?\"), the t value is computed as:\n",
    "\n",
    "$ t(X_1, X_2) = \\frac{\\bar{X_1} - \\bar{X_2}}{s}$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\bar{X_1}$ is the mean of sample $X_1$\n",
    "* s is the standard error of the difference, which is:\n",
    "\n",
    "$e_1 = \\frac{\\sigma_1}{\\sqrt(n_1)}$\n",
    "\n",
    "Where $\\sigma_1$ is the standard deviation of $X_1$ and $n_1$ is the number of observations in $X_1$, This is the \"standard error\" of $X_1$.\n",
    "\n",
    "Then,\n",
    "\n",
    "$s = \\sqrt{e_1^2 + e_2^2}$\n",
    "\n",
    "Which is the standard error of the difference between the means.\n",
    "\n",
    "The output of your code should be a pandas DataFrame where the index values are the p-values we are testing (i.e. 0.01, 0.05, 0.1, 0.2) and the columns are the degrees-of-freedom, which is how many data points in both $X_1$ and $X_2$, subtracting 2. Values to compute are 5, 10, 20, 50, 100 (and so on if you are inclined).\n",
    "\n",
    "The values can be computed via simulation - that is, draw many random samples, and compute the likelihood of getting a t value at least that high between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For solutions, see `solutions/simulation_ttest.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
